Release Notes:  wave_glwu v1.1.1 - released to NCO on November 28, 2021

Transition from WCOSS Cray to WCOSS2 (update for run time optimization).

Where is the release tag on subversion/git/vlab?
https://github.com/NOAA-EMC/wave_glwu.git Tag: IT-wave_glwu.v1.1.1-20211128

List of external software used (anything outside of your vertical structure), 
including compilers and version numbers for everything. Software used must be 
a minimal list of modules/versions specified per job:

jwave_glwu_prep:
envvar/1.0
prod_envir/2.0.5
prod_util/2.0.8
PrgEnv-intel/8.1.0
craype/2.7.8
intel/19.1.3.304
cray-mpich/8.1.7
cray-pals/1.0.12
cfp/2.0.4
wgrib2/2.0.8
hdf5/1.10.6

jwave_glwu_forecast:
envvar/1.0
prod_envir/2.0.5
prod_util/2.0.8
PrgEnv-intel/8.1.0
craype/2.7.8
intel/19.1.3.304
cray-mpich/8.1.7
cray-pals/1.0.12
hdf5/1.10.6
netcdf/4.7.4
jasper/2.0.25
libpng/1.6.37
zlib/1.2.11
g2/3.4.5
w3nco/2.4.1
bacio/2.4.1

jwave_glwu_post:
envvar/1.0
prod_envir/2.0.5
prod_util/2.0.8
PrgEnv-intel/8.1.0
craype/2.7.8
intel/19.1.3.304
cfp/2.0.4
wgrib2/2.0.8
grib_util/1.2.2
libjpeg/9c
hdf5/1.10.6
netcdf/4.7.4

jwave_glwu_pgen:
envvar/1.0
prod_envir/2.0.5
prod_util/2.0.8
PrgEnv-intel/8.1.0
craype/2.7.8
intel/19.1.3.304
libjpeg/9c
libpng/1.6.37
grib_util/1.2.2
wgrib2/2.0.8
util_shared/1.4.0

List of all code/scripts modified with this release (relative to v1.1.0)

versions/build.ver
versions/run.ver
modulefiles/build_wavewatch3.modules
sorc/multiwavefcst.fd/w3triamd.F90
sorc/multiwavefcst.fd/makefile
ecf/jwave_glwu_prep.ecf
ecf/jwave_glwu_forecast.ecf
ecf/jwave_glwu_post.ecf
ecf/jwave_glwu_pgen.ecf

For development, please ignore:
dev/lsf_scripts/JWAVE_GLWU_FORECAST.lsf
dev/lsf_scripts/JWAVE_GLWU_POST.lsf
dev/lsf_scripts/JWAVE_GLWU_PREP.lsf
dev/lsf_scripts/glwu.cron
dev/lsf_scripts/glwu.boss.lsf

What changes were made to the above code/scripts to support the new architecture?
Relative to v1.1.0, the forecast job was sped up by allocating more nodes, cores 
and RAM. This was done due to code profiling by GDIT showing a memory bottleneck:

Old:
#PBS -l select=6:ncpus=128:mem=150GB
export mpicmd='mpiexec -n 768 -ppn 128'

New:
#PBS -l select=12:ncpus=128:mem=500GB
export mpicmd='mpiexec -n 768 -ppn 64 --depth 2 --cpu-bind depth'

Were any other changes made that arenâ€™t directly related to the transition?
No

Are there any changes to incoming data needs or outgoing products? 
No

If output filenames are changing, list the current and new filename
N/A

Compute resource information, for every job:
*** Providing PBS and/or ecFlow submission scripts as part of release tag is preferred; 
if they are provided then resource information is not needed in the release notes.
See wave_glwu/ecf/

Runtime changes compared to current production (/gpfs/dell1/nco/ops/com/logs/runtime/daily/* for current stats)
Long cycles (01z, 07z, 13z, 19z):
jwave_glwu_prep:       00:02:50   (prod: 00:00:21)
jwave_glwu_forecast:   00:22:23   (prod: 00:24:34)
jwave_glwu_post:       00:19:44   (prod: 00:15:09)
jwave_glwu_pgen:       00:00:10   (prod: 00:00:27)
Short cycles (remaining hours: 00z, 02z, 03z, 04z, 05z, 06z, 08z, 09z, 10z, 11z, 12z, etc.):
jwave_glwu_prep        00:02:30   (prod: 00:00:20)
jwave_glwu_forecast    00:08:42   (prod: 00:08:31)
jwave_glwu_post        00:11:12   (prod: 00:09:31)
jwave_glwu_pgen        00:00:11   (prod: 00:00:11)

Disk space required per day or per cycle; data retention on disk will remain the same unless otherwise requested.
Unchanged at 118G/day.

Dissemination will remain the same unless otherwise communicated to NCO and a PNS/SCN issued
Unchanged.

HPSS archive retention will remain the same unless approval granted by HPCRAC
Unchanged.

What are your up and downstream dependencies?
Unchanged. Upstream: NDFD winds and NIC sea ice analysis from DCOM. Downstream: None.

Provide a pointer to your COMOUT directory that was used during testing: 
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/GLWU/COM_dev/glwu.20211003
Canned data from WCOSS1 Cray for comparison:
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/GLWU/COM_wcoss1_prod/glwu.20211003
